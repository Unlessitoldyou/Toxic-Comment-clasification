{"cells":[{"cell_type":"markdown","metadata":{"id":"2gNrsdWLgLVR"},"source":["# **DSCI 691: Project Implementation**\n","# Title: Wikipedia Toxic Comments Classification"]},{"cell_type":"markdown","metadata":{"id":"yLWMzgn0gS_x"},"source":["## General Group / Submission Information\n","\n","### Project group\n","- Group member 1\n","    - Name: Nikhil Muthuvenkatesh\n","    - Email: nm3297@drexel.edu\n","- Group member 2\n","    - Name: Seyi Oyesiku\n","    - Email: so536@drexel.edu\n","- Group member 3\n","    - Name: Connor Roth\n","    - Email: cxr25@drexel.edu\n","- Group member 4\n","    - Name: Amira Bendjama\n","    - Email: ab4745@drexel.edu\n","\n","### Additional submission comments\n","- External support and stakeholders: NA\n","- Other (other): NA\n","- Large groups justification: NA"]},{"cell_type":"markdown","metadata":{"id":"tXB-mb0FgpIv"},"source":["## Project Overview\n","### Project goals\n","The main goal of our project is to develop a model that can effectively detect and classify different types of toxic comments - these include threats, obscenity, insults, and identity-based hate. Specifically, we aim to achieve a higher F1/F-measure score than the best performing model from our research paper of interest. As a result, we hope to use the model to improve online conversation by reducing the prevalence of toxic comments that discourage open discussion and limit the exchange of different perspectives.\n","\n","\n","### NLP task description\n","This project's task is essentially an exercise in text-based classification, which has been a traditional NLP task since the early days of machine learning. Specifically, as mentioned above, the task is a multi-class classification exericse to correctly bucket Wikipedia comments into one of 6 \"toxic\" categories. We will certainly need to employ pre-processing of some sort (Word2Vec or GloVe) to create word embeddings for our models to use.\n","\n","\n","### Project data\n","The single [dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data) we plan to leverage is a collection of of Wikipedia comments labeled by human raters for toxic behavior. The types of toxicity are:\n","- toxic\n","- severe_toxic\n","- obscene\n","- threat\n","- insult\n","- identity_hate\n","\n","__Files__:\n","\n","train.csv - the training set contains comments with their binary labels.\n","\n","test.csv - the test set contains some comments.\n","\n","sample_submission.csv - a sample submission file in the correct format.\n","\n","test_labels.csv - labels for the test data.\n","\n","### Neural methodology\n","For this project, we will experiment with different neural architectures and hyperparameters to find the best-performing model for our task.\n","\n","Specifically, we will attempt to use a combination of the following four types of models:\n","1. Convolutional neural network (CNN)\n","2. Bidirectional long short-term memory (LSTM)\n","3. Bidirectional gated recurrent unit (GRU)\n","4. Google BERT AI\n","\n","Our team's hope is to experiment with these models individually and as ensemble groupings to identify the best possible combinations. Also, we must note that the architecture (# of neurons, # of layers, etc.) will likely be experimented with versus the baseline model.\n","\n","### Baselines\n","As mentioned above, the primary baseline we intend to compare to is that of the research paper we used in Section 2. Its best performing model was an ensemble of CNN ensemble, Bidirectional LSTM, and Bidirectional GRU. So, we must outperform the [\"0.828 and\n","0.872 F1-score for toxic/non-toxic classification and toxicity\n","types prediction respectively\"](https://ieeexplore.ieee.org/document/8614166) in order for this research to be considered successful.\n","\n","### Evaluation\n","In our project, we will use F1-score as our primary performance metric for evaluating our neural models. This is because accuracy and area under the ROC curve (ROC AUC) can be biased or mask poor performance in cases where data is imbalanced, as is the case with our toxic comment classification task. F1-score is the harmonic mean of precision and recall, providing a reliable measure of the model's performance even with skewed data. By using F1-score, we can assess our models' ability to accurately identify toxic comments, which is critical for deploying a functional application that can help improve online conversation by effectively detecting and managing toxic content."]},{"cell_type":"markdown","metadata":{"id":"TsbPHdSqiPTM"},"source":["## Relevant Libraries / General Configuration"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36830,"status":"ok","timestamp":1686704367973,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"ZTeC02EjgXnw","outputId":"4976a5c1-bea5-4bec-a737-82f8856de577"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1YLfeHvPldAiwpi8QnKgiHwSAugt6ZWXH/Project Implementation\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Nikhil File Path\n","# %cd \"/content/drive/My Drive/DSCI_691/Project Implementation\"\n","\n","# Amira Change Directory Command #\n","%cd \"/content/drive/My Drive/Project Implementation\"\n","\n","# Seyi Change Directory Command #\n","# %cd \"/content/drive/MyDrive/Project Implementation\"\n","\n","# CXR Change Directory Command #\n","# %cd \"/content/drive/My Drive/DSCI_691/Project Implementation\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16676,"status":"ok","timestamp":1686704384634,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"Z6brWlTMMNSL","outputId":"5b7b1df7-f4e7-4148-93ab-7a5d2ab304a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FGziQz2FiZwS","executionInfo":{"status":"ok","timestamp":1686704402655,"user_tz":240,"elapsed":18027,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["# Data import / processing\n","import pandas as pd\n","import re\n","\n","# Analysis\n","import numpy as np\n","\n","# Preprocessing - General\n","from keras.preprocessing import text\n","from keras.utils import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","#Preprocessing - LSTM\n","from torchtext.vocab import GloVe\n","\n","# Preprocessing - BERT\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Modeling - General\n","import torch\n","import torch.nn as nn\n","import tensorflow as tf\n","\n","# Modeling - LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n","\n","# Modeling - BERT\n","from transformers import BertTokenizer\n","from transformers import BertModel\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","# Model Evaluation\n","from sklearn.metrics import classification_report"]},{"cell_type":"markdown","metadata":{"id":"hrGdTCM8hkQl"},"source":["## Import Training and Test Data\n","\n","The dataset for this project is contained within a series of CSVs that have already been downloaded from [Kaggle](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge).\n","\n","Thus, the process will simply be to load these datasets using pandas' \"read_csv\" function."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-LE66sJ9JpPj","executionInfo":{"status":"ok","timestamp":1686704408282,"user_tz":240,"elapsed":5669,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Create relative path and read using pandas\n","train = pd.read_csv(\"./data/train.csv\")\n","test = pd.read_csv(\"./data/test.csv\")\n","test_labels = pd.read_csv(\"./data/test_labels.csv\")"]},{"cell_type":"code","source":["# # ATTEMPT: reducing data\n","# np.random.seed(42)\n","# num_rows_to_remove = int(0.8 * len(train))\n","# rows_to_remove = np.random.choice(train.index, num_rows_to_remove, replace=False)\n","# reduced_train = train.drop(rows_to_remove)\n","# reduced_train = reduced_train.reset_index(drop=True)\n","# reduced_train.to_csv(\"./data/reduced_train.csv\", index=False)\n","# train = reduced_train"],"metadata":{"id":"hGorS-wDRVdq","executionInfo":{"status":"ok","timestamp":1686704408282,"user_tz":240,"elapsed":39,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1686704408283,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"k9AHJmbVJsgV","outputId":"f293d73c-efe5-48dc-af2c-ebfcb5e6655c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "],"text/html":["\n","  <div id=\"df-185c63c5-a8ee-4ebf-95ca-dec505e308dd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-185c63c5-a8ee-4ebf-95ca-dec505e308dd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-185c63c5-a8ee-4ebf-95ca-dec505e308dd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-185c63c5-a8ee-4ebf-95ca-dec505e308dd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["#Examine head of training dataset\n","train.head()"]},{"cell_type":"markdown","metadata":{"id":"VDJvoEu_JwrB"},"source":["Here, we see that there are 8 total columns:\n","- id: A unique identifier representing the individual comment\n","- comment_text: the body of the comment, which will serve as the data underlying the team's modeling efforts.\n","- toxic, severe_toxic, obscene, threat, insult, identity_hate: Labels representing the classification task. We will further explore these during the EDA portion of the project."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1686704408283,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"AMEeVBrJJtGW","outputId":"287b914c-6b4d-4774-fb76-23fdbeb513a1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text\n","0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n","1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n","2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n","3  00017563c3f7919a  :If you have a look back at the source, the in...\n","4  00017695ad8997eb          I don't anonymously edit articles at all."],"text/html":["\n","  <div id=\"df-4545cc22-1c5b-425a-9feb-97c71de24263\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00001cee341fdb12</td>\n","      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000247867823ef7</td>\n","      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00013b17ad220c46</td>\n","      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00017563c3f7919a</td>\n","      <td>:If you have a look back at the source, the in...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017695ad8997eb</td>\n","      <td>I don't anonymously edit articles at all.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4545cc22-1c5b-425a-9feb-97c71de24263')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4545cc22-1c5b-425a-9feb-97c71de24263 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4545cc22-1c5b-425a-9feb-97c71de24263');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["#Examine head of the test dataset\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"jqBozzrIJ0O_"},"source":["Test only contains ID and comment_text. The correct labels are stored in \"test_labels\", which we will merge in now to get a complete picture of the dataframe."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_5dNAmZ-YnY-","executionInfo":{"status":"ok","timestamp":1686704409208,"user_tz":240,"elapsed":947,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["test = test.merge(right = test_labels, on = \"id\", how = \"inner\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1686704409210,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"Y2OQBc__Y_d7","outputId":"82d243c7-d7bc-4ca8-8795-5dfad949f494"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text  toxic  \\\n","0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...     -1   \n","1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...     -1   \n","2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...     -1   \n","3  00017563c3f7919a  :If you have a look back at the source, the in...     -1   \n","4  00017695ad8997eb          I don't anonymously edit articles at all.     -1   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0            -1       -1      -1      -1             -1  \n","1            -1       -1      -1      -1             -1  \n","2            -1       -1      -1      -1             -1  \n","3            -1       -1      -1      -1             -1  \n","4            -1       -1      -1      -1             -1  "],"text/html":["\n","  <div id=\"df-1e21805c-c510-4b81-9e32-b0a27354c91d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00001cee341fdb12</td>\n","      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000247867823ef7</td>\n","      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00013b17ad220c46</td>\n","      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00017563c3f7919a</td>\n","      <td>:If you have a look back at the source, the in...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017695ad8997eb</td>\n","      <td>I don't anonymously edit articles at all.</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e21805c-c510-4b81-9e32-b0a27354c91d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e21805c-c510-4b81-9e32-b0a27354c91d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e21805c-c510-4b81-9e32-b0a27354c91d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["#Examine head of merged test dataset\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"5DcS8AdhZDOX"},"source":["Notice that the first few rows all have -1 values. These represent items that are not scored as part of the task. So, we'll want to remove these and keep only the ones for which we have all labels."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"2jsq4bduZPZC","executionInfo":{"status":"ok","timestamp":1686704409211,"user_tz":240,"elapsed":126,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["cond_1 = test[\"toxic\"] != -1\n","cond_2 = test[\"severe_toxic\"] != -1\n","cond_3 = test[\"obscene\"] != -1\n","cond_4 = test[\"threat\"] != -1\n","cond_5 = test[\"insult\"] != -1\n","cond_6 = test[\"identity_hate\"] != -1\n","\n","test = test[cond_1 & cond_2 & cond_3 & cond_4 & cond_5 & cond_6].reset_index(drop=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1686704409212,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"O5Uf5QOjZyIL","outputId":"1debd2b6-a625-4272-cc3f-f24e07e58662"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n","1  000247e83dcc1211                   :Dear god this site is horrible.      0   \n","2  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n","3  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n","4  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "],"text/html":["\n","  <div id=\"df-55b0254f-e6ff-4e72-b0d4-f344e9ee7cec\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001ea8717f6de06</td>\n","      <td>Thank you for understanding. I think very high...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000247e83dcc1211</td>\n","      <td>:Dear god this site is horrible.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0002f87b16116a7f</td>\n","      <td>\"::: Somebody will invariably try to add Relig...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0003e1cccfd5a40a</td>\n","      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00059ace3e3e9a53</td>\n","      <td>\" \\n\\n == Before adding a new product to the l...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55b0254f-e6ff-4e72-b0d4-f344e9ee7cec')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-55b0254f-e6ff-4e72-b0d4-f344e9ee7cec button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-55b0254f-e6ff-4e72-b0d4-f344e9ee7cec');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["test.head()"]},{"cell_type":"markdown","metadata":{"id":"AUXMXn-RYrWc"},"source":["Before we move on, though, let's now take a look at the number of instances in the training and test sets."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1686704409213,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"3XdPZwKYLQA3","outputId":"cdf7bfe8-b1af-4552-ef63-a1514bb2daa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of comments in TRAIN:  159571\n","Number of comments in TEST:  63978\n"]}],"source":["print(\"Number of comments in TRAIN: \", len(train))\n","print(\"Number of comments in TEST: \", len(test))"]},{"cell_type":"markdown","metadata":{"id":"qQuJ0XyuN3Um"},"source":["Training has a little over 150,000 comments. Test has roughly 64,000 comments after cleaning up for the comments that were not part of the competition scoring. Initially, it had close to 150,000 as well."]},{"cell_type":"markdown","metadata":{"id":"8R2q41h5iGkY"},"source":["## Exploratory Data Analysis\n","\n","### Review Training Comment Examples\n","\n","Next, we will take a closer look at comments to better understand the kind of data we will be working with."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1686704409215,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"XKnyPR79iFjB","outputId":"261e4f94-44c0-4171-d956-7abace80e8c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["train[\"comment_text\"][2]"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1686704409216,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"GVJGLup3J52U","outputId":"82adc421-b8a2-4c71-ff10-1613c06763ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"The Mitsurugi point made no sense - why not argue to include Hindi on Ryo Sakazaki's page to include more information?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["train[\"comment_text\"][18]"]},{"cell_type":"markdown","metadata":{"id":"HodieuR7KD1X"},"source":["Here, we can see the context of the data. Specifically, these derive from users on Wikipedia posting comments related to edits / content.\n","\n","Let's now filter down to some actual \"toxic\" comments to see what those look like. Specifically, we'll refrain from looking at some of the more \"obscene\" ones."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1686704409544,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"guZporLVKEuy","outputId":"608fa4b1-f8c9-443f-dc17-225756867eb6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Bye! \\n\\nDon't look, come or think of comming back! Tosser.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["#Create \"toxic\" filter\n","train_toxic_filter = train[train[\"toxic\"] == 1].reset_index()\n","\n","train_toxic_filter[\"comment_text\"][2]"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1686704409545,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"Fw0Wxq5BKJXf","outputId":"fd400226-0f0f-4e55-b4a6-82ac64f6804c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"You should be fired, you're a moronic wimp who is too lazy to do research. It makes me sick that people like you exist in this world.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["train_toxic_filter[\"comment_text\"][17]"]},{"cell_type":"markdown","metadata":{"id":"pEIDdpeLKONg"},"source":["### Comment Length Descriptive Statistics\n","\n","With some initial views of the comments out of the way, we will dig into the specifics of commments with some descriptive statistics on length."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1686704409546,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"ruwYghMXKPAa","outputId":"8bd06aae-8393-485f-98fc-ed9d38fe9af5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Comment Length:  394.0732213246768\n","Mean Comment Length:  205.0\n","Standard Deviation of Comment Length:  590.7202819048919\n","Min Comment Length:  6\n","Max Comment Length:  5000\n"]}],"source":["#Grab comment lengths from training dataset\n","comment_lengths = train.comment_text.str.len()\n","\n","#Analyze mean, median, standard deviation, min, and max values\n","print(\"Mean Comment Length: \", comment_lengths.mean())\n","print(\"Mean Comment Length: \", comment_lengths.median())\n","print(\"Standard Deviation of Comment Length: \", comment_lengths.std())\n","print(\"Min Comment Length: \", comment_lengths.min())\n","print(\"Max Comment Length: \", comment_lengths.max())"]},{"cell_type":"markdown","metadata":{"id":"TC4uHAvuKXkT"},"source":["The main insights are as follows:\n","- Comments are, on average, just shy of 400 characters long.\n","- The distribution has a right tail, with some very large comments reaching up to 5000 max.\n","- Following on this, the distribution itself has a very large standard deviation - just shy of 600 characters.\n","\n","Let's take a look at this graphically, though."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":706,"status":"ok","timestamp":1686704410152,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"L2d8k88nKYav","outputId":"7f42ff5e-02a7-4f7a-a737-f0b21fb01ceb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["#Graph a histogram using the comment_lengths\n","comment_lengths.plot(kind = \"hist\",\n","                     title = \"Histogram of Wikipedia Comment Character Lengths\",\n","                     xlabel = \"Comment Character Lengths\");"]},{"cell_type":"markdown","metadata":{"id":"Du5---IvK7tq"},"source":["### Exploration of Labels\n","\n","Next, we'll take a look at the labels themselves to understand...\n","1. Descriptive statistics\n","2. Interaction between labels"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":508,"status":"ok","timestamp":1686704410599,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"pSWSvaOgK47d","outputId":"656fe6e2-a247-4501-b49f-fe8775fff089"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               toxic   severe_toxic        obscene         threat  \\\n","count  159571.000000  159571.000000  159571.000000  159571.000000   \n","mean        0.095844       0.009996       0.052948       0.002996   \n","std         0.294379       0.099477       0.223931       0.054650   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","              insult  identity_hate           none  \n","count  159571.000000  159571.000000  159571.000000  \n","mean        0.049364       0.008805       0.898321  \n","std         0.216627       0.093420       0.302226  \n","min         0.000000       0.000000       0.000000  \n","25%         0.000000       0.000000       1.000000  \n","50%         0.000000       0.000000       1.000000  \n","75%         0.000000       0.000000       1.000000  \n","max         1.000000       1.000000       1.000000  "],"text/html":["\n","  <div id=\"df-ba910b6f-9fb3-454d-a889-f59f6a101dde\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>none</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","      <td>159571.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.095844</td>\n","      <td>0.009996</td>\n","      <td>0.052948</td>\n","      <td>0.002996</td>\n","      <td>0.049364</td>\n","      <td>0.008805</td>\n","      <td>0.898321</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.294379</td>\n","      <td>0.099477</td>\n","      <td>0.223931</td>\n","      <td>0.054650</td>\n","      <td>0.216627</td>\n","      <td>0.093420</td>\n","      <td>0.302226</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba910b6f-9fb3-454d-a889-f59f6a101dde')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ba910b6f-9fb3-454d-a889-f59f6a101dde button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ba910b6f-9fb3-454d-a889-f59f6a101dde');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["# Create list of label columns\n","label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","# Create new label of \"none\" for better understanding of what the rest of the dataset looks like\n","train_new = train.copy()\n","train_new['none'] = 1-train_new[label_cols].max(axis=1)\n","\n","# Append \"none\"\n","label_cols.append(\"none\")\n","\n","#Show descriptive statistics\n","train_new.describe()"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174,"status":"ok","timestamp":1686704410600,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"2OIVEv8DTvV_","outputId":"49975481-191a-4fba-87cf-8904289b14b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sum of all values:  1.118273370474585\n","Sum of 'toxic' and 'none':  0.9941656065325153\n"]}],"source":["#Sum mean values\n","print(\"Sum of all values: \", sum([train_new[x].mean() for x in train_new[label_cols]]))\n","print(\"Sum of 'toxic' and 'none': \", train_new[\"toxic\"].mean() + train_new[\"none\"].mean())"]},{"cell_type":"markdown","metadata":{"id":"7sseJmVzO8Wa"},"source":["A few important insights come out of this view:\n","- The labels overlap. That is, adding the mean values across all labels (including \"none\") will yield over 100%\n","- Toxic + none alone do NOT add to 100%. It's just under that value.\n","\n","So, we will need to examine this overlap further to tease out what exactly is going on here."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":144,"status":"ok","timestamp":1686704410602,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"XG4Q2tHwPUq-","outputId":"d147d7ab-0e61-4bd5-ba4c-6d960b098333"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["none       0       1\n","toxic               \n","0        931  143346\n","1      15294       0"],"text/html":["\n","  <div id=\"df-f8b473ca-9041-43b8-be4c-54f91752901a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>none</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","    <tr>\n","      <th>toxic</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>931</td>\n","      <td>143346</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15294</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8b473ca-9041-43b8-be4c-54f91752901a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f8b473ca-9041-43b8-be4c-54f91752901a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f8b473ca-9041-43b8-be4c-54f91752901a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}],"source":["pd.crosstab(train_new.toxic, train_new.none)"]},{"cell_type":"markdown","metadata":{"id":"h5d-pRU9P76U"},"source":["As mentioned earlier, there are 931 comments that are neither \"toxic\" nor \"none\". So, let's take a look at what exactly these are."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1686704410779,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"_Mhip-25QEZf","outputId":"c7ccc0e4-abcd-4816-f8a4-ee2d05332fd7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["toxic            0.000000\n","severe_toxic     0.000000\n","obscene          0.561762\n","threat           0.031149\n","insult           0.572503\n","identity_hate    0.110634\n","none             0.000000\n","dtype: float64"]},"metadata":{},"execution_count":22}],"source":["#Create conditions\n","condition1 = train_new[\"toxic\"] == 0\n","condition2 = train_new[\"none\"] == 0\n","\n","#Apply conditions as masks\n","train_label_exploration = train_new[condition1 & condition2]\n","\n","#Check mean values\n","train_label_exploration[label_cols].mean()"]},{"cell_type":"markdown","metadata":{"id":"KSsjdMi1RJHy"},"source":["Those that aren't \"toxic\" or \"none\" are a mix of \"obscene\", \"threat\", \"insult\", and \"identity_hate\". Again, note that these add up to more than 1, indicating overlap of these labels.\n","\n","This is important, as it means that we need to treat thsi as a multi-label classification task when predicting for toxic comment labels. Further, it would behoove us to split the modeling effort into two distinct parts - one for toxic vs. non-toxic (binary classification) and one for types of toxic (multi-label classification)."]},{"cell_type":"markdown","metadata":{"id":"6Ttuu_3DRWXq"},"source":["### Searching for Nulls / Missing Values\n","\n","Now, we will turn our attention to check for any null or NA values in the dataframe. Any of these could be problematic, so we must be thorough and examine the entirety of the dataframe for both Train and Test."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1686704411100,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"GDOLSv2ORZYt","outputId":"29eb9333-737b-4135-e497-56e568c2b864"},"outputs":[{"output_type":"stream","name":"stdout","text":["Any TRAIN Null Values?:  False\n","Any TRAIN NA Values?:  False\n","\n","Any TEST Null Values?:  False\n","Any TEST NA Values?:  False\n"]}],"source":["print(\"Any TRAIN Null Values?: \", train.isnull().values.any())\n","print(\"Any TRAIN NA Values?: \", train.isna().values.any())\n","print()\n","print(\"Any TEST Null Values?: \", test.isnull().values.any())\n","print(\"Any TEST NA Values?: \", test.isna().values.any())"]},{"cell_type":"markdown","metadata":{"id":"GBrq_BmASnBs"},"source":["Thankfully, this is not the case for either Train or Test, so we can proceed without the need to fill values / adjust the dataframes."]},{"cell_type":"markdown","metadata":{"id":"nz7g6IlDQROQ"},"source":["## Preprocessing\n","\n","### General Preprocessing\n","First, we will create the necessary additional labels in the training and test datasets for Binary Classification."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Kcm0XjAEjQ_M","executionInfo":{"status":"ok","timestamp":1686704411101,"user_tz":240,"elapsed":10,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["def roll_up_labels(df):\n","  # Create list of label columns\n","  label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","  #Create new \"toxic_rollup\"\n","  df[\"toxic_rollup\"] = df[label_cols].max(axis=1)\n","\n","  return df"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"7Ygw7_majZ7I","executionInfo":{"status":"ok","timestamp":1686704411460,"user_tz":240,"elapsed":367,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["train = roll_up_labels(train)\n","test = roll_up_labels(test)"]},{"cell_type":"markdown","metadata":{"id":"xXgxJ1KUjIJj"},"source":["Next, we want to create a function that can more generally clean up some of the text and transform it into something useable."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Ae-fVpXoQQpU","executionInfo":{"status":"ok","timestamp":1686704411461,"user_tz":240,"elapsed":12,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["def text_preprocessing(text):\n","    # Remove any characters that are not letters or punctuation marks\n","    cleaned_text = re.sub(r\"[^a-zA-Z\\s.,!?']\", \"\", text)\n","\n","    # Remove any extra whitespace\n","    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n","\n","    return cleaned_text.lower()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1686704411461,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"Kr5-jM2FQaP2","outputId":"08c96f6f-cb93-4555-b295-2b88bcec902f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Sentence:  Explanation\n","Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n","Preprocessed Sentence:  explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now....\n"]}],"source":["# Testing out the function\n","print('Original Sentence: ', train['comment_text'][0])\n","print('Preprocessed Sentence: ', text_preprocessing(train['comment_text'][0]))"]},{"cell_type":"markdown","metadata":{"id":"E3dYsNVDSKjI"},"source":["We see that it's removed some unnecessary spaces / line breaks and cleaned up other odds and ends.\n","\n","Now, we will process both our training and test datasets using this text preprocessing function."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"46rUDWymR0cF","executionInfo":{"status":"ok","timestamp":1686704432909,"user_tz":240,"elapsed":21454,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Create a new column by applying this to the train and test datasets\n","train[\"processed_comment_text\"] = train[\"comment_text\"].apply(text_preprocessing)\n","test[\"processed_comment_text\"] = test[\"comment_text\"].apply(text_preprocessing)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1686704432910,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"JYG_rwYyT09S","outputId":"841813da-2fcf-4754-ed0b-324d757d4775"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  toxic_rollup  \\\n","0             0        0       0       0              0             0   \n","1             0        0       0       0              0             0   \n","2             0        0       0       0              0             0   \n","3             0        0       0       0              0             0   \n","4             0        0       0       0              0             0   \n","\n","                              processed_comment_text  \n","0  explanation why the edits made under my userna...  \n","1  d'aww! he matches this background colour i'm s...  \n","2  hey man, i'm really not trying to edit war. it...  \n","3   more i can't make any real suggestions on imp...  \n","4  you, sir, are my hero. any chance you remember...  "],"text/html":["\n","  <div id=\"df-2077a1bc-f4b9-4465-9c77-79e632be1243\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>toxic_rollup</th>\n","      <th>processed_comment_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>explanation why the edits made under my userna...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>d'aww! he matches this background colour i'm s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>hey man, i'm really not trying to edit war. it...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>more i can't make any real suggestions on imp...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>you, sir, are my hero. any chance you remember...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2077a1bc-f4b9-4465-9c77-79e632be1243')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2077a1bc-f4b9-4465-9c77-79e632be1243 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2077a1bc-f4b9-4465-9c77-79e632be1243');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":29}],"source":["train.head()"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1686704432910,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"},"user_tz":240},"id":"mkWGaYCbVDoT","outputId":"612a835a-b9a4-414a-978e-e8902622aee1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n","1  000247e83dcc1211                   :Dear god this site is horrible.      0   \n","2  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n","3  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n","4  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  toxic_rollup  \\\n","0             0        0       0       0              0             0   \n","1             0        0       0       0              0             0   \n","2             0        0       0       0              0             0   \n","3             0        0       0       0              0             0   \n","4             0        0       0       0              0             0   \n","\n","                              processed_comment_text  \n","0  thank you for understanding. i think very high...  \n","1                    dear god this site is horrible.  \n","2   somebody will invariably try to add religion?...  \n","3   it says it right there that it is a type. the...  \n","4   before adding a new product to the list, make...  "],"text/html":["\n","  <div id=\"df-11f11b8d-d211-4493-8fd4-9d20f77f8912\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>toxic_rollup</th>\n","      <th>processed_comment_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001ea8717f6de06</td>\n","      <td>Thank you for understanding. I think very high...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>thank you for understanding. i think very high...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000247e83dcc1211</td>\n","      <td>:Dear god this site is horrible.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dear god this site is horrible.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0002f87b16116a7f</td>\n","      <td>\"::: Somebody will invariably try to add Relig...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>somebody will invariably try to add religion?...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0003e1cccfd5a40a</td>\n","      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>it says it right there that it is a type. the...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00059ace3e3e9a53</td>\n","      <td>\" \\n\\n == Before adding a new product to the l...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>before adding a new product to the list, make...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11f11b8d-d211-4493-8fd4-9d20f77f8912')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-11f11b8d-d211-4493-8fd4-9d20f77f8912 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-11f11b8d-d211-4493-8fd4-9d20f77f8912');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":30}],"source":["test.head()"]},{"cell_type":"markdown","metadata":{"id":"ryXDFoUqVGKQ"},"source":["Finally, we need to parse the dataframes into their useable components - the X and Y variables for train and test."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"-iCkuxgvVFql","executionInfo":{"status":"ok","timestamp":1686704432912,"user_tz":240,"elapsed":95,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["X_train = train[\"processed_comment_text\"].values\n","X_test = test[\"processed_comment_text\"].values\n","\n","#Creating BINARY task labels\n","Y_train_binary = train[\"toxic_rollup\"].values\n","Y_test_binary = test[\"toxic_rollup\"].values\n","\n","#Creating MULTILABEL task labels\n","Y_train_multilabel = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n","Y_test_multilabel = test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"]},{"cell_type":"markdown","metadata":{"id":"Y2n73vEeQyiL"},"source":["## Bi-LSTM Preprocessing\n","\n","First, we need to implement tokenization on the training and test datasets."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"HF1Lv4pnQ34i","executionInfo":{"status":"ok","timestamp":1686704457837,"user_tz":240,"elapsed":25019,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Set max features to 100K\n","max_features=100000\n","\n","#Instantiate tokenizer\n","tok=text.Tokenizer(num_words=max_features, lower=True)\n","\n","#Fit tokenizer on text from X_train\n","tok.fit_on_texts(X_train)"]},{"cell_type":"markdown","metadata":{"id":"xKA8u8PSnq7T"},"source":["Next, we will convert these texts to sequences and pad to a given length. Here, the max length will be 300."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ktc2lZFLo8qR","executionInfo":{"status":"ok","timestamp":1686704466652,"user_tz":240,"elapsed":8842,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Convert texts to sequences\n","X_train_LSTM = tok.texts_to_sequences(X_train)\n","X_test_LSTM = tok.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"-4YB9DV9lXRd","executionInfo":{"status":"ok","timestamp":1686704469169,"user_tz":240,"elapsed":2539,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Set max len to 300\n","maxlen=300\n","\n","#Pad sequences to max length\n","X_train_LSTM = pad_sequences(X_train_LSTM, maxlen = maxlen)\n","X_test_LSTM = pad_sequences(X_test_LSTM, maxlen = maxlen)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"L2r5EzFHCOFU","executionInfo":{"status":"ok","timestamp":1686704469520,"user_tz":240,"elapsed":366,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["### BINARY CLASSIFICATION ###\n","X_train_LSTM_binary, X_val_LSTM_binary, Y_train_LSTM_binary, Y_val_LSTM_binary = \\\n","train_test_split(X_train_LSTM, Y_train_binary, test_size = 0.1, random_state = 42)\n","\n","### MULTILABEL CLASSIFICATION ###\n","X_train_LSTM_multilabel, X_val_LSTM_multilabel, Y_train_LSTM_multilabel, Y_val_LSTM_multilabel = \\\n","train_test_split(X_train_LSTM, Y_train_multilabel, test_size = 0.1, random_state = 42)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"OG1SFnqmDMoq","executionInfo":{"status":"ok","timestamp":1686704469522,"user_tz":240,"elapsed":7,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Set batch size of 32\n","batch_size = 32\n","\n","\n","### BINARY CLASSIFICATION ###\n","\n","# Convert to tensors\n","train_inputs_LSTM_binary = torch.tensor(X_train_LSTM_binary)\n","val_inputs_LSTM_binary = torch.tensor(X_val_LSTM_binary)\n","train_labels_LSTM_binary = torch.tensor(Y_train_LSTM_binary).float()\n","val_labels_LSTM_binary = torch.tensor(Y_val_LSTM_binary).float()\n","\n","# Train: Create Dataloaders\n","train_data_LSTM_binary = TensorDataset(train_inputs_LSTM_binary, train_labels_LSTM_binary)\n","train_sampler_LSTM_binary = RandomSampler(train_data_LSTM_binary)\n","train_dataloader_LSTM_binary = DataLoader(train_data_LSTM_binary, sampler=train_sampler_LSTM_binary, batch_size=batch_size)\n","\n","# Val: Create Dataloaders\n","val_data_LSTM_binary = TensorDataset(val_inputs_LSTM_binary, val_labels_LSTM_binary)\n","val_sampler_LSTM_binary = SequentialSampler(val_data_LSTM_binary)\n","val_dataloader_LSTM_binary = DataLoader(val_data_LSTM_binary, sampler=val_sampler_LSTM_binary, batch_size=batch_size)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"3oCzLccZEO7a","executionInfo":{"status":"ok","timestamp":1686704469917,"user_tz":240,"elapsed":189,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["### MULTILABEL CLASSIFICATION ###\n","\n","# Convert to tensors\n","train_inputs_LSTM_multilabel = torch.tensor(X_train_LSTM_multilabel)\n","val_inputs_LSTM_multilabel = torch.tensor(X_val_LSTM_multilabel)\n","train_labels_LSTM_multilabel = torch.tensor(Y_train_LSTM_multilabel).float()\n","val_labels_LSTM_multilabel = torch.tensor(Y_val_LSTM_multilabel).float()\n","\n","# Train: Create Dataloaders\n","train_data_LSTM_multilabel = TensorDataset(train_inputs_LSTM_multilabel, train_labels_LSTM_multilabel)\n","train_sampler_LSTM_multilabel = RandomSampler(train_data_LSTM_multilabel)\n","train_dataloader_LSTM_multilabel = DataLoader(train_data_LSTM_multilabel, sampler=train_sampler_LSTM_multilabel, batch_size=batch_size)\n","\n","# Val: Create Dataloaders\n","val_data_LSTM_multilabel = TensorDataset(val_inputs_LSTM_multilabel, val_labels_LSTM_multilabel)\n","val_sampler_LSTM_multilabel = SequentialSampler(val_data_LSTM_multilabel)\n","val_dataloader_LSTM_multilabel = DataLoader(val_data_LSTM_multilabel, sampler=val_sampler_LSTM_multilabel, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"XDCgP7tLqUvm"},"source":["Now that we have appropriately tokenized, converted to sequences, and padded to a standard length, we can create our embeddings and word indexes. This will allow us to prepare the embedding matrix."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"iBCyLXh6qj-D","executionInfo":{"status":"ok","timestamp":1686704708943,"user_tz":240,"elapsed":239030,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Embedding file path\n","embed_fp = \"./data/glove.840B.300d.txt\"\n","\n","#Create an empty embeddings index dictionary\n","embeddings_index = {}\n","\n","#Iterate through all the global vectors, populating the embeddings index\n","with open(embed_fp, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Jr6B5kjksfvP","executionInfo":{"status":"ok","timestamp":1686704708945,"user_tz":240,"elapsed":42,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Create word index from tokenizer\n","word_index = tok.word_index\n","\n","#Gather number of words (min of features and word_index)\n","num_words = min(max_features, len(word_index) + 1)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"_xJ6h10Os2Ef","executionInfo":{"status":"ok","timestamp":1686704709515,"user_tz":240,"elapsed":608,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#Set embed size\n","embed_size = 300\n","\n","#Create zero'd embedding matrix\n","embedding_matrix = np.zeros((num_words, embed_size))\n","\n","#Iterate through word index items\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:  # NOTE: Words not in embedding will be all zeroes\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"markdown","metadata":{"id":"UPCY5N5DQvuY"},"source":["## BERT Preprocessing\n","First, we must get the BERT tokenizer, as this is crucial to removing \"the out-of-vocabulary obstacle\" and compressing vocab to a more manageable size (see Week 10 Notes, Section 10.1.1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9Zhy1pbQx-U"},"outputs":[],"source":["BERT_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"markdown","metadata":{"id":"hrTsRsUqfGH8"},"source":["Next, we check to see if GPU(s) are available - if yes, we'll use them. If not, we'll proceed with CPU usage."]},{"cell_type":"markdown","metadata":{"id":"ygIicnLbfQB1"},"source":["The next component is to preprocess for BERT specifically\n","    Add special tokens to the start and end of each sentence.\n","    Pad & truncate all sentences to a single constant length.\n","    Explicitly differentiate real tokens from padding tokens with the “attention mask”.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uATcv9xgRkSz"},"outputs":[],"source":["# Creating preprocessing function for BERT\n","def preprocessing_for_BERT(data):\n","  MAX_LEN = 300\n","\n","  #Creating empty lists for input ids and attention masks\n","  input_ids = []\n","  attention_masks = []\n","\n","  # Preprocessing each sentence\n","  for sentence in data:\n","    #Use tokenizer to \"encode plus\" with relevant parameters, including returning an attention mask\n","    encoded_sentence = BERT_tokenizer.encode_plus(text = text_preprocessing(sentence), max_length = MAX_LEN,\n","                                             pad_to_max_length = True, truncation = True, return_attention_mask = True)\n","\n","    #Append the input ids and attention masks\n","    input_ids.append(encoded_sentence.get('input_ids'))\n","    attention_masks.append(encoded_sentence.get('attention_mask'))\n","\n","  #Turn both the input ids and attention masks into tensors\n","  input_ids = torch.tensor(input_ids)\n","  attention_maks = torch.tensor(attention_masks)\n","\n","\n","  return input_ids, attention_masks"]},{"cell_type":"markdown","metadata":{"id":"QtWN89EngnJx"},"source":["Before we run this preprocessing, though, we'll need to split apart the X train into train/validation splits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAV8s1jVgXA2"},"outputs":[],"source":["### BINARY CLASSIFICATION ###\n","X_train_BERT, X_val_BERT, Y_train_BERT_binary, Y_val_BERT_binary = train_test_split(X_train, Y_train_binary, test_size = 0.1, random_state = 42)\n","\n","### MULTILABEL CLASSIFICATION ###\n","X_train_BERT, X_val_BERT, Y_train_BERT_multilabel, Y_val_BERT_multilabel = train_test_split(X_train, Y_train_multilabel, test_size = 0.1, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"3BX8ACDoirCu"},"source":["Now, we can apply BERT-specific preprocessing for all of the X variables."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261720,"status":"ok","timestamp":1686697539826,"user":{"displayName":"Connor X","userId":"11302055682030278437"},"user_tz":240},"id":"23hmQN1fRrPu","outputId":"ba45b0e2-8771-4611-8136-7a4e161efabf"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["train_inputs_BERT, train_masks_BERT = preprocessing_for_BERT(X_train_BERT)\n","val_inputs_BERT, val_masks_BERT = preprocessing_for_BERT(X_val_BERT)\n","test_inputs_BERT, test_masks_BERT = preprocessing_for_BERT(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YY3PYidn_DB"},"outputs":[],"source":["train_masks_BERT = torch.tensor(train_masks_BERT)\n","val_masks_BERT = torch.tensor(val_masks_BERT)\n","test_masks_BERT = torch.tensor(test_masks_BERT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npS49OpnifK2"},"outputs":[],"source":["#Set batch size of 32 - can be 16 or 32 for BERT\n","batch_size = 16\n","\n","### BINARY CLASSIFICATION ###\n","\n","# Convert labels to tensors\n","train_labels_BERT_binary = torch.tensor(Y_train_BERT_binary)\n","val_labels_BERT_binary = torch.tensor(Y_val_BERT_binary)\n","\n","# Train: Create Dataloaders\n","train_data_BERT_binary = TensorDataset(train_inputs_BERT, train_masks_BERT, train_labels_BERT_binary)\n","train_sampler_BERT_binary = RandomSampler(train_data_BERT_binary)\n","train_dataloader_BERT_binary = DataLoader(train_data_BERT_binary, sampler=train_sampler_BERT_binary, batch_size=batch_size)\n","\n","# Val: Create Dataloaders\n","val_data_BERT_binary = TensorDataset(val_inputs_BERT, val_masks_BERT, val_labels_BERT_binary)\n","val_sampler_BERT_binary = SequentialSampler(val_data_BERT_binary)\n","val_dataloader_BERT_binary = DataLoader(val_data_BERT_binary, sampler=val_sampler_BERT_binary, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0Bx3QQKkzjT"},"outputs":[],"source":["### MULTILABEL CLASSIFICATION ###\n","\n","# Convert labels to tensors\n","train_labels_BERT_multilabel = torch.tensor(Y_train_BERT_multilabel)\n","val_labels_BERT_multilabel = torch.tensor(Y_val_BERT_multilabel)\n","\n","# Train: Create Dataloaders\n","train_data_BERT_multilabel = TensorDataset(train_inputs_BERT, train_masks_BERT, train_labels_BERT_multilabel)\n","train_sampler_BERT_multilabel = RandomSampler(train_data_BERT_multilabel)\n","train_dataloader_BERT_multilabel = DataLoader(train_data_BERT_multilabel, sampler=train_sampler_BERT_multilabel, batch_size=batch_size)\n","\n","# Val: Create Dataloaders\n","val_data_BERT_multilabel = TensorDataset(val_inputs_BERT, val_masks_BERT, val_labels_BERT_multilabel)\n","val_sampler_BERT_multilabel = SequentialSampler(val_data_BERT_multilabel)\n","val_dataloader_BERT_multilabel = DataLoader(val_data_BERT_multilabel, sampler=val_sampler_BERT_multilabel, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"Vci7y6X0iJ2d"},"source":["## Modeling: Binary Classification\n","### Bidirectional LSTM (Binary Classification) - Keras Implementation\n","\n","Here, we will build a Keras implementation of a Bi-LSTM. The reason we've constructed this model is that the PyTorch implementation has memory issues that seem to have been avoided in this implementation."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"r55UsfG4aqRv","executionInfo":{"status":"ok","timestamp":1686704736623,"user_tz":240,"elapsed":27115,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}}},"outputs":[],"source":["#KERAS PROCESSING\n","tokenizer = text.Tokenizer(num_words=10000)\n","tokenizer.fit_on_texts(X_train)\n","\n","train_sequences = tokenizer.texts_to_sequences(X_train)\n","test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","max_length = 300\n","train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n","test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","\n","k_X_train_LSTM_binary, k_X_val_LSTM_binary, k_Y_train_LSTM_binary, k_Y_val_LSTM_binary = train_test_split(train_padded,  Y_train_binary, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"90nR-LViuGnr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686707614846,"user_tz":240,"elapsed":2877471,"user":{"displayName":"Amira Bendjama","userId":"17622145886376606568"}},"outputId":"a7472821-2189-424a-c35d-91df672adb24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","3990/3990 [==============================] - 1290s 321ms/step - loss: 0.1483 - accuracy: 0.9486 - f1_score: 0.6271 - val_loss: 0.1102 - val_accuracy: 0.9604 - val_f1_score: 0.7242\n","Epoch 2/2\n","3990/3990 [==============================] - 1280s 321ms/step - loss: 0.1009 - accuracy: 0.9633 - f1_score: 0.7455 - val_loss: 0.1034 - val_accuracy: 0.9633 - val_f1_score: 0.7619\n","2000/2000 [==============================] - 124s 62ms/step - loss: 0.2062 - accuracy: 0.9176 - f1_score: 0.6303\n","Validation Accuracy: 0.9176123142242432 f1 :  0.6302759051322937\n","2000/2000 [==============================] - 125s 62ms/step\n"]}],"source":["#KERAS MODELING\n","from keras import backend as K\n","\n","def f1_score(y_true, y_pred):\n","    y_pred = K.round(y_pred)  # Convert predictions to binary values (0 or 1)\n","    tp = K.sum(K.round(y_true * y_pred))  # True positives\n","    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))  # False positives\n","    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))  # False negatives\n","\n","    precision = tp / (tp + fp + K.epsilon())  # Adding a small epsilon to avoid division by zero\n","    recall = tp / (tp + fn + K.epsilon())\n","\n","    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","    return f1_score\n","\n","\n","model = Sequential()\n","model.add(Embedding(10000, 16, input_length=max_length))\n","model.add(Bidirectional(LSTM(32, dropout=0.2)))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n","model.fit(k_X_train_LSTM_binary,  k_Y_train_LSTM_binary, epochs=2 , validation_data = (k_X_val_LSTM_binary,k_Y_val_LSTM_binary))\n","_, accuracy , f1 = model.evaluate(test_padded, Y_test_binary)\n","print('Validation Accuracy:', accuracy , \"f1 : \", f1 )\n","\n","#predictions on test data\n","predictions = model.predict(test_padded)\n","\n","#convert predictions to binary labels\n","binary_predictions = np.where(predictions >= 0.5, 1, 0)"]},{"cell_type":"markdown","source":["### Bidirectional LSTM (Binary Classification) - PyTorch Implementation\n","\n","Here is our PyTorch implementation, which does not train properly due to memory issues. We have retained this as documentation that we've given consideration to the modeling and training portions of the model (despite it not training properly)."],"metadata":{"id":"p1CLnT6H9RDk"}},{"cell_type":"code","source":["class BinaryLSTM(nn.Module):\n","    def __init__(self, max_features, embed_size, embedding_matrix, hidden_dim, lstm_layers, dropout=0.1, bidirectional=True):\n","        super(BinaryLSTM, self).__init__()\n","\n","        self.max_features = max_features\n","        self.embed_size = embed_size\n","        self.embedding_matrix = embedding_matrix\n","        self.hidden_dim = hidden_dim\n","        self.lstm_layers = lstm_layers\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        # Layers\n","        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n","        self.embedding.weight = nn.Parameter(torch.tensor(self.embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False\n","        self.lstm = nn.LSTM(self.embed_size, self.hidden_dim, num_layers=self.lstm_layers, bidirectional=self.bidirectional, batch_first=True)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(self.dropout)\n","        self.linear = nn.Linear(self.hidden_dim * self.num_directions, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        #Embedding\n","        embedding = self.embedding(x)\n","        embedding = torch.squeeze(torch.unsqueeze(embedding, 0))\n","\n","        #LSTM layer\n","        lstm, _ = self.lstm(embedding)\n","\n","        relu = self.relu(lstm)\n","\n","        #Dropout layer\n","        drop = self.dropout(relu)\n","\n","        #Linear layer\n","        linear = self.linear(drop)\n","\n","        #Sigmoid activation\n","        out = self.sigmoid(linear)\n","        return out\n","\n","def train_model_binary(model, train_loader, val_loader, lr=1e-3, batch_size=32, max_epochs=10, patience=5):\n","    torch.manual_seed(42)\n","\n","    # Building the Adam Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    # Building the Loss Function\n","    criterion = nn.BCELoss()\n","\n","    epoch = 0\n","    no_imp = 0\n","    best_val_loss = None\n","    while epoch < max_epochs and no_imp < patience:\n","        print('=' * 50)\n","        print('Begin epoch', epoch + 1)\n","        print('-' * 50)\n","\n","        #TRAINING PART OF LOOP\n","        train_loss = 0\n","        model.train()\n","\n","        for x in train_loader:\n","            optimizer.zero_grad()\n","            output = model(x[0])\n","            loss = criterion(output, x[1])\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        print(f'Current train loss: {train_loss:.2f}')\n","\n","        #VALIDATION PART OF LOOP\n","        val_loss = 0\n","        model.eval()\n","        for x in val_loader:\n","            optimizer.zero_grad()\n","            output = model(x[0])\n","            loss = criterion(output, x[1])\n","            loss.backward()\n","            optimizer.step()\n","            val_loss += loss.item()\n","\n","        print(f'Total validation loss: {val_loss:.2f}')\n","\n","        if best_val_loss is None or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_imp = 0\n","            print(\"best model saved\")\n","            torch.save(model.state_dict(), './data/Binary_LSTM.pt')\n","            best_model = model\n","        else:\n","            no_imp += 1\n","\n","        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n","\n","        epoch += 1\n","        print('=' * 50)\n","\n","    # Reload and return the best model\n","\n","    return model.load_state_dict(torch.load('./data/Binary_LSTM.pt'))\n"],"metadata":{"id":"k0VdcCtgcgtS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["in_dim = 300\n","hidden_dim = 256\n","out_dim = 1\n","lstm_layers = 2\n","\n","Binary_BiLSTM = BinaryLSTM(max_features, embed_size, embedding_matrix, hidden_dim, lstm_layers, dropout = 0.2, bidirectional = True)\n","Binary_BiLSTM"],"metadata":{"id":"LieBK3t9cpIr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686697612139,"user_tz":240,"elapsed":2045,"user":{"displayName":"Connor X","userId":"11302055682030278437"}},"outputId":"ddc7071a-0809-4293-ec42-07aca0f9fd73"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BinaryLSTM(\n","  (embedding): Embedding(100000, 300)\n","  (lstm): LSTM(300, 256, num_layers=50, batch_first=True, bidirectional=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (linear): Linear(in_features=512, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["lr=1e-2\n","batch_size = 32\n","max_epochs = 2\n","\n","best_binary_bilstm = train_model_binary(Binary_BiLSTM, train_dataloader_LSTM_binary,\n","                                     val_dataloader_LSTM_binary, lr = lr,\n","                                     batch_size = batch_size, max_epochs = max_epochs)"],"metadata":{"id":"fGT6ESyEc1Rr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d15d29f-04e1-4a1e-990c-cee27b431fce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","Begin epoch 1\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"xICo8mevRFUN"},"source":["### BERT (Binary Classification)\n","\n","Next,we move on to the BERT Binary Classifier. Here, again, we had issues with memory exceptions preventing us from running the model. So, we kept the architecture in place to demonstrate the thought and effort put into the project."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryWziJNnRLim"},"outputs":[],"source":["class BertBinaryClassifier(nn.Module):\n","    def __init__(self, freeze_bert=False):\n","        super(BertBinaryClassifier, self).__init__()\n","\n","        D_in, H, D_out = 256, 6, 2\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"]},{"cell_type":"code","source":["in_dim = 758\n","hidden_dim = 256\n","out_dim = 1\n","\n","bert_binary_classifier = BertBinaryClassifier()\n","\n","print(bert_binary_classifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":983,"referenced_widgets":["95a548e80c0c440a9cab162b1e20bb79","bf06d3ba20574591ab0e361997d91de8","8ee2879d7cb44ed9b74c05f3bbead009","53197ed1552b478cb029554c63c82f0e","92c82c209f324bb88a319a821fe863cb","c84bd1a21bc641978915c95e0aee235e","dc0d4525b03b4023b69ffbb52272ecb5","5854c99dccbe4bc283af957e29cba477","81a66aba0f5c4864abb5520844c2b4eb","cf2d564d8a1f47d29dbf6667a39a6b67","fbd53f364ed24c548ee4782450c507d9"]},"id":"Mxpb-Gy25mv4","executionInfo":{"status":"ok","timestamp":1686697628657,"user_tz":240,"elapsed":10654,"user":{"displayName":"Connor X","userId":"11302055682030278437"}},"outputId":"463c31ec-9ba2-40b8-ea83-6bfdd1e0c0b5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a548e80c0c440a9cab162b1e20bb79"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["BertBinaryClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=256, out_features=6, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=6, out_features=2, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["def train_binary_model(model, train_loader, val_loader, lr=1e-2, batch_size=32, max_epochs=10, patience=5):\n","    torch.manual_seed(42)\n","\n","    # Building the Adam Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    # Building the Loss Function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    epoch = 0\n","    no_imp = 0\n","    best_val_loss = None\n","    while epoch < max_epochs and no_imp < patience:\n","        print('=' * 50)\n","        print('Begin epoch', epoch + 1)\n","        print('-' * 50)\n","\n","        # TRAINING PART OF LOOP\n","        train_loss = 0\n","        model.train()\n","\n","        for input_ids, attention_mask, labels in train_loader:\n","            optimizer.zero_grad()\n","            logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        print(f'Current train loss: {train_loss:.2f}')\n","\n","        # VALIDATION PART OF LOOP\n","        val_loss = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for input_ids, attention_mask, labels in val_loader:\n","                logits = model(input_ids, attention_mask)\n","                loss = criterion(logits, labels)\n","                val_loss += loss.item()\n","\n","        print(f'Total validation loss: {val_loss:.2f}')\n","\n","        if best_val_loss is None or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_imp = 0\n","            torch.save(model(), './data/binary_BERT.pt')\n","        else:\n","            no_imp += 1\n","\n","        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n","\n","        epoch += 1\n","        print('=' * 50)\n","\n","    torch.train_model_binary(torch.load('./data/binary_BERT.pt'))\n","    return model"],"metadata":{"id":"FFzL_kgAQ48r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_binary_classifier = BertBinaryClassifier(freeze_bert = False)\n","epochs = 4\n","bert_binary_classifier.to(device)\n","optimizer = AdamW(bert_binary_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","total_steps = len(train_dataloader_BERT_binary) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)"],"metadata":{"id":"kKzJ5f-LPhje","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686698002568,"user_tz":240,"elapsed":4524,"user":{"displayName":"Connor X","userId":"11302055682030278437"}},"outputId":"7c6d6668-c391-4a9c-915d-eb51adbc98f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["best_Binary_bert = train_bert(bert_binary_classifier, train_dataloader_BERT_binary,\n","                                     val_dataloader_BERT_binary, lr = 5e-5,\n","                                     batch_size = 32, max_epochs = 2)"],"metadata":{"id":"Fj7PneQ9CNlL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4TwhHUS1fiI"},"source":["## Modeling: Multilabel Classification\n","\n","\n","### Bidirectional LSTM (Multilabel Classification)\n","\n","Now, we move on to multilabel classification modeling. Here, the implementation is nearly the same as for binary, but this implementation worked correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJGLrrDlxnex"},"outputs":[],"source":["class BiLSTM(torch.nn.Module):\n","\n","    def __init__(self, max_features, embed_size, embedding_matrix,\n","                 hidden_dim, out_dim, lstm_layers, drop=0.1, bidirectional = True):\n","\n","        super(BiLSTM, self).__init__()\n","\n","        #Parameters coming in\n","        self.max_features = max_features\n","        self.embed_size = embed_size\n","        self.embedding_matrix = embedding_matrix\n","        self.hidden_dim = hidden_dim\n","        self.out_dim = out_dim\n","        self.lstm_layers = lstm_layers\n","        self.drop = drop\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        #LAYERS\n","        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n","        self.embedding.weight = nn.Parameter(torch.tensor(self.embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False\n","        self.lstm = nn.LSTM(self.embed_size, self.hidden_dim, bidirectional=True, batch_first=True)\n","        self.linear = nn.Linear(self.hidden_dim*4, self.hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(self.drop)\n","        self.linear2 = nn.Linear(self.hidden_dim, self.out_dim)\n","        self.out = nn.Sigmoid()\n","\n","\n","    def forward(self, x):\n","        #Embedding\n","        h_embedding = self.embedding(x)\n","        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n","\n","        #LSTM layer\n","        h_lstm, _ = self.lstm(h_embedding)\n","\n","        #Average and Max Pool, Concatenate\n","        avg_pool = torch.mean(h_lstm, 1)\n","        max_pool, _ = torch.max(h_lstm, 1)\n","        conc = torch.cat(( avg_pool, max_pool), 1)\n","\n","        #ReLU activation\n","        linear1 = self.linear(conc)\n","        relu = self.relu(linear1)\n","\n","        #Dropout layer\n","        drop = self.dropout(relu)\n","\n","        #Linear layer\n","        linear2 = self.linear2(drop)\n","\n","        #Sigmoid activation\n","        out = self.out(linear2)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HVmKmtEvGMo"},"outputs":[],"source":["in_dim = maxlen\n","hidden_dim = 256\n","out_dim = 6\n","lstm_layers = 2\n","\n","biLSTM_multilabel = BiLSTM(max_features, embed_size, embedding_matrix, hidden_dim, out_dim, lstm_layers, 0.1, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1686697706678,"user":{"displayName":"Connor X","userId":"11302055682030278437"},"user_tz":240},"id":"AjLugvGe1rBc","outputId":"d7afdf7b-4ef2-40a9-9faa-25649c1a704c"},"outputs":[{"output_type":"stream","name":"stdout","text":["BiLSTM(\n","  (embedding): Embedding(100000, 300)\n","  (lstm): LSTM(300, 256, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=1024, out_features=256, bias=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (linear2): Linear(in_features=256, out_features=6, bias=True)\n","  (out): Sigmoid()\n",")\n"]}],"source":["print(biLSTM_multilabel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXZTzldh8m4Y"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, lr=1e-2, batch_size=32, max_epochs=10, patience=5):\n","    torch.manual_seed(42)\n","\n","    # Building the Adam Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    # Building the Loss Function\n","    criterion = nn.BCELoss()\n","\n","    epoch = 0\n","    no_imp = 0\n","    best_val_loss = None\n","    while epoch < max_epochs and no_imp < patience:\n","        print('=' * 50)\n","        print('Begin epoch', epoch + 1)\n","        print('-' * 50)\n","\n","        #TRAINING PART OF LOOP\n","        train_loss = 0\n","        model.train()\n","\n","        for x in train_loader:\n","            optimizer.zero_grad()\n","            output = model(x[0])\n","            loss = criterion(output, x[1])\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        print(f'Current train loss: {train_loss:.2f}')\n","\n","        #VALIDATION PART OF LOOP\n","        val_loss = 0\n","        model.eval()\n","        for x in val_loader:\n","            optimizer.zero_grad()\n","            output = model(x[0])\n","            loss = criterion(output, x[1])\n","            loss.backward()\n","            optimizer.step()\n","            val_loss += loss.item()\n","\n","        print(f'Total validation loss: {val_loss:.2f}')\n","\n","        if best_val_loss is None or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_imp = 0\n","            torch.save(model, './data/Toxic_Multilabel_bilstm.pt')\n","        else:\n","            no_imp += 1\n","\n","        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n","\n","\n","        epoch += 1\n","        print('=' * 50)\n","\n","    # Reload and return the best model\n","    return torch.load('./data/Toxic_Multilabel_bilstm.pt')"]},{"cell_type":"code","source":["lr = lr=1e-2\n","batch_size = 32\n","max_epochs = 2\n","\n","best_multilabel_bilstm = train_model(biLSTM_multilabel, train_dataloader_LSTM_multilabel,\n","                                     val_dataloader_LSTM_multilabel, lr = lr,\n","                                     batch_size = batch_size, max_epochs = max_epochs)"],"metadata":{"id":"KBkPcI3mCJ43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eivVssGSM9Mg"},"source":["### BERT (Multilabel Classification)\n","\n","Here, we have modified the BERT classifier slightly from its binary implementation. However, this approach still has yielded a memory exception and thus we have not been able to train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JA77eOkMcv3"},"outputs":[],"source":["class BertClassifier(nn.Module):\n","  def __init__(self, freeze_bert=False):\n","    super(BertClassifier, self).__init__()\n","\n","    D_in, H, D_out = 768, 50, 6\n","\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","    self.classifer = nn.Sequential(\n","        nn.Linear(D_in, H),\n","        nn.ReLU(),\n","        nn.Linear(H, D_out),\n","        nn.Sigmoid()\n","    )\n","\n","    if freeze_bert:\n","      for param in self.bert.parameters():\n","        param.requires_grad = False\n","\n","  def forward(self, input_ids, attention_mask):\n","      outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","      last_hidden_state_cls = outputs[0][:, 0, :]\n","      probs = self.classifier(last_hidden_state_cls)\n","\n","      return probs"]},{"cell_type":"code","source":["def train_bert(model, train_loader, val_loader, lr=1e-2, batch_size=32, max_epochs=10, patience=5):\n","    torch.manual_seed(42)\n","\n","    # Building the Adam Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    # Building the Loss Function\n","    criterion = nn.BCELoss()\n","\n","    epoch = 0\n","    no_imp = 0\n","    best_val_loss = None\n","    while epoch < max_epochs and no_imp < patience:\n","        print('=' * 50)\n","        print('Begin epoch', epoch + 1)\n","        print('-' * 50)\n","\n","        #TRAINING PART OF LOOP\n","        train_loss = 0\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","        model.train()\n","\n","        for batch in train_loader:\n","            batch_loss = 0\n","            batch_counts += 1\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","            optimizer.zero_grad()\n","            output = model(b_input_ids, b_attn_mask)\n","            loss = criterion(output, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","            print(f'Current batch train loss: {batch_loss:.2f}')\n","\n","        print(f'Current train loss: {train_loss:.2f}')\n","\n","        #VALIDATION PART OF LOOP\n","        val_loss = []\n","        val_accuracy = []\n","        model.eval()\n","        for x in val_loader:\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","            with torch.no_grad():\n","              output = model(b_input_ids, b_attn_mask)\n","            loss = criterion(output, b_labels)\n","            val_loss.append(loss.item())\n","            preds = torch.argmax(output, dim=1).flatten()\n","            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","            val_accuracy.append(accuracy)\n","        val_loss = np.mean(val_loss)\n","        val_accuracy = np.mean(val_accuracy)\n","\n","        print(f'Total validation loss: {val_loss:.2f}')\n","\n","        if best_val_loss is None or val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_imp = 0\n","            torch.save(model, './data/Toxic_Multilabel_bert.pt')\n","        else:\n","            no_imp += 1\n","\n","        print(f'Best validation loss: {best_val_loss:.2f} (Epochs without improvement: {no_imp})')\n","\n","\n","        epoch += 1\n","        print('=' * 50)\n","\n","    # Reload and return the best model\n","    return torch.load('./data/Toxic_Multilabel_bert.pt')"],"metadata":{"id":"0sjBVWBpVgxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_multi_classifier = BertClassifier(freeze_bert = False)\n","epochs = 4\n","bert_multi_classifier.to(device)\n","optimizer = AdamW(bert_multi_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","total_steps = len(train_dataloader_BERT_multilabel) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaQGU9KVbGCH","outputId":"03801fee-4f36-4ebe-a2a4-f5213cd177ee","executionInfo":{"status":"ok","timestamp":1686698100297,"user_tz":240,"elapsed":3722,"user":{"displayName":"Connor X","userId":"11302055682030278437"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["best_multilabel_bert = train_bert(bert_multi_classifier, train_dataloader_BERT_multilabel,\n","                                     val_dataloader_BERT_multilabel, lr = 5e-5,\n","                                     batch_size = 16, max_epochs = 2)"],"metadata":{"id":"74Ozv7pVCBly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-NEeWKrQL6vW"},"source":["## Evaluation: Multilabel Classification\n","\n","Finally, we will evaluate the results of the multilabel bi-LSTM (the only PyTorch implementation that successfully ran)."]},{"cell_type":"code","source":["best_multilabel_bilstm = torch.load('./data/Toxic_Multilabel_bilstm.pt')\n","\n","# Create list of label columns\n","label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"],"metadata":{"id":"OGAPIcORxV4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_uvEiBUL7_o"},"outputs":[],"source":["def eval_tagger(model, batch_loader, threshold, label_names=None):\n","    model.eval()\n","\n","    preds = []; golds = []\n","    for batch in batch_loader:\n","        #Gather labels from batch\n","        labels = batch[1]\n","\n","        #Gather outputs, turn into predictions\n","        predicted = []\n","        outputs = model(batch[0])\n","        for sample in outputs:\n","          predicted.append([1 if i > threshold else 0 for i in sample])\n","\n","        #Extend preds and golds with numpy versions of predicted and labels\n","        preds.extend(np.array(predicted))\n","        golds.extend(np.array(labels))\n","\n","    return classification_report(golds, preds, target_names=label_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44FvjWmYSOi7"},"outputs":[],"source":["### MULTILABEL CLASSIFICATION ###\n","\n","# Convert to tensors\n","test_inputs_LSTM_multilabel = torch.tensor(X_test_LSTM)\n","test_labels_LSTM_multilabel = torch.tensor(Y_test_multilabel).float()\n","\n","# Test: Create Dataloaders\n","test_data_LSTM_multilabel = TensorDataset(test_inputs_LSTM_multilabel, test_labels_LSTM_multilabel)\n","test_sampler_LSTM_multilabel = SequentialSampler(test_data_LSTM_multilabel)\n","test_dataloader_LSTM_multilabel = DataLoader(test_data_LSTM_multilabel, sampler=test_sampler_LSTM_multilabel, batch_size=batch_size)"]},{"cell_type":"code","source":["for fold, dx in [('train', train_dataloader_LSTM_multilabel), ('val', val_dataloader_LSTM_multilabel), ('test', test_dataloader_LSTM_multilabel)]:\n","    print(f'Evaluating {fold}')\n","    print(eval_tagger(best_multilabel_bilstm, dx, threshold = 0.5, label_names=label_cols))"],"metadata":{"id":"PK3-0zHcCgt_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["UPCY5N5DQvuY"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"95a548e80c0c440a9cab162b1e20bb79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf06d3ba20574591ab0e361997d91de8","IPY_MODEL_8ee2879d7cb44ed9b74c05f3bbead009","IPY_MODEL_53197ed1552b478cb029554c63c82f0e"],"layout":"IPY_MODEL_92c82c209f324bb88a319a821fe863cb"}},"bf06d3ba20574591ab0e361997d91de8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c84bd1a21bc641978915c95e0aee235e","placeholder":"​","style":"IPY_MODEL_dc0d4525b03b4023b69ffbb52272ecb5","value":"Downloading model.safetensors: 100%"}},"8ee2879d7cb44ed9b74c05f3bbead009":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5854c99dccbe4bc283af957e29cba477","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81a66aba0f5c4864abb5520844c2b4eb","value":440449768}},"53197ed1552b478cb029554c63c82f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf2d564d8a1f47d29dbf6667a39a6b67","placeholder":"​","style":"IPY_MODEL_fbd53f364ed24c548ee4782450c507d9","value":" 440M/440M [00:05&lt;00:00, 80.8MB/s]"}},"92c82c209f324bb88a319a821fe863cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c84bd1a21bc641978915c95e0aee235e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc0d4525b03b4023b69ffbb52272ecb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5854c99dccbe4bc283af957e29cba477":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81a66aba0f5c4864abb5520844c2b4eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf2d564d8a1f47d29dbf6667a39a6b67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbd53f364ed24c548ee4782450c507d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}